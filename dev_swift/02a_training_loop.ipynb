{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/home/ubuntu/notebooks/swift/FastaiNotebooks\")\n",
      "\t\tFastaiNotebooks\n",
      "Working in: /tmp/tmpxgu193tm\n",
      "Fetching https://github.com/mxcl/Path.swift\n",
      "Fetching https://github.com/JustHTTP/Just\n",
      "Completed resolution in 1.67s\n",
      "Cloning https://github.com/JustHTTP/Just\n",
      "Resolving https://github.com/JustHTTP/Just at 0.7.1\n",
      "Cloning https://github.com/mxcl/Path.swift\n",
      "Resolving https://github.com/mxcl/Path.swift at 0.16.2\n",
      "Compile Swift Module 'Just' (1 sources)\n",
      "Compile Swift Module 'Path' (9 sources)\n",
      "Compile Swift Module 'FastaiNotebooks' (3 sources)\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "Installation complete!"
     ]
    }
   ],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebooks\")' FastaiNotebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward and backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let mnist = MnistDataset(path: Path.home/\".fastai\"/\"data\"/\"mnist_tst\")\n",
    "var x_train = mnist.xTrain\n",
    "var y_train = mnist.yTrain\n",
    "var x_valid = mnist.xValid\n",
    "var y_valid = mnist.yValid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (n,m) = (Int(x_train.shape[0]),Int(x_train.shape[1]))\n",
    "let c = y_train.max()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MyModel: Layer {\n",
    "    var layer1 = Dense<Float>(inputSize: m, outputSize: nh, activation: relu)\n",
    "    var layer2 = Dense<Float>(inputSize: nh, outputSize: 1)\n",
    "    \n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        return input.sequenced(in: context, through: layer1, layer2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let trainingContext = Context(learningPhase: .training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let y_trainf = Tensor<Float>(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let optimizer = SGD<MyModel, Float>(learningRate: 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Batch: TensorGroup{\n",
    "    let x: Tensor<Float>\n",
    "    let y: Tensor<Float>\n",
    "    \n",
    "    static var _typeList: [TensorDataType] = [\n",
    "        Float.tensorFlowDataType,\n",
    "        Float.tensorFlowDataType\n",
    "    ]\n",
    "    static var _unknownShapeList: [TensorShape?] = [nil, nil]\n",
    "    func _unpackTensorHandles(into address: UnsafeMutablePointer<CTensorHandle>?) {\n",
    "        address!.advanced(by: 0).initialize(to: x.handle._cTensorHandle)\n",
    "        address!.advanced(by: 1).initialize(to: y.handle._cTensorHandle)\n",
    "    }\n",
    "    init(_owning tensorHandles: UnsafePointer<CTensorHandle>?) {\n",
    "        x = Tensor(handle: TensorHandle(_owning: tensorHandles!.advanced(by: 0).pointee))\n",
    "        y = Tensor(handle: TensorHandle(_owning: tensorHandles!.advanced(by: 1).pointee))\n",
    "    }\n",
    "    \n",
    "    init(x: Tensor<Float>, y: Tensor<Float>){\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let train_ds:Dataset<Batch> = Dataset(elements:Batch(x:x_train, y:y_trainf)).batched(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "<Cell 18>:8:53: note: function is differentiable only with respect to a smaller subset of arguments\n        let (loss, grads) = model.valueWithGradient { model -> Tensor<Float> in\n                                                    ^\n\nerror: <Cell 18>:8:53: error: function is not differentiable\n        let (loss, grads) = model.valueWithGradient { model -> Tensor<Float> in\n                                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n<Cell 18>:10:20: note: expression is not differentiable\n            return loss_func(preds, batch.y)\n                   ^\n\n"
     ]
    }
   ],
   "source": [
    "func basic_training_loop<Model, Opt:Optimizer> (train_ds:Dataset<Batch>, model: inout Model, opt: inout Opt, \n",
    "                         loss_func: @differentiable (Tensor<Float>, Tensor<Float>)->Tensor<Float>)\n",
    "                         where Opt.Model == Model, Opt.Scalar == Float,\n",
    "                               Model.Input == Tensor<Float>,\n",
    "                               Model.Output == Tensor<Float>\n",
    "{\n",
    "    for batch in train_ds{\n",
    "        let (loss, grads) = model.valueWithGradient { model -> Tensor<Float> in\n",
    "            let preds = model.applied(to: batch.x, in: trainingContext)\n",
    "            return loss_func(preds, batch.y)\n",
    "        }\n",
    "        print(loss)\n",
    "        opt.update(&model.allDifferentiableVariables, along: grads)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "error: <Cell 20>:13:27: error: generic parameter 'Self' could not be inferred\n        opt.update(&model.allDifferentiableVariables, along: grads)\n                          ^\n\n"
     ]
    }
   ],
   "source": [
    "func basic_training_loop<Model, Opt:Optimizer> (train_ds:Dataset<Batch>, model: inout Model, opt: inout Opt, \n",
    "                         loss_func: (Tensor<Float>, Tensor<Float>)->Tensor<Float>)\n",
    "                         where Opt.Model == Model, Opt.Scalar == Float,\n",
    "                               Model.Input == Tensor<Float>,\n",
    "                               Model.Output == Tensor<Float>\n",
    "{\n",
    "    for batch in train_ds{\n",
    "        let (loss, grads) = model.valueWithGradient(at: batch.x) { (model, y) -> Tensor<Float> in\n",
    "            let preds = model.applied(to: batch.x, in: trainingContext)\n",
    "            return loss_func(preds, y)\n",
    "        }\n",
    "        print(loss)\n",
    "        opt.update(&model.allDifferentiableVariables, along: grads)\n",
    "    }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
